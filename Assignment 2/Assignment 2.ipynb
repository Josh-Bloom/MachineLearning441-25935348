{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401687fa-4db7-4960-959b-94c760ccb6eb",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e94f1a-0d77-4b38-b4e7-38e94e48b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10005815-99a4-432a-9f6e-40e50e809ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('forestCover.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ba62e-6603-4245-818c-d490d4cff346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04df3b-fcab-4360-9d76-3549cbcc5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87f037-a61a-4327-a077-0e0f9b4c2343",
   "metadata": {},
   "source": [
    "## Data Quality Issue Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092f735-df41-4a58-aa29-6eb4350bd14b",
   "metadata": {},
   "source": [
    "### Encoding of textual entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f518e-d02b-4c3b-8276-2f6989b1e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns.tolist()\n",
    "idx1, idx2 = cols.index('Water_Level'), cols.index('Observation_ID')\n",
    "cols[idx1], cols[idx2] = cols[idx2], cols[idx1]\n",
    "data.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d196b61-fd89-4cef-9fb4-701c7b6550e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Soil_Type1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5bba7-1b60-40ff-918f-46b9631b6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Soil_Type1'] = data['Soil_Type1'].map(lambda x : 1 if x=='positive' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768be832-f1a6-4f8a-85f5-8ae0129ef1bd",
   "metadata": {},
   "source": [
    "### Missing value analysis and correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff9778-79a9-4deb-8a61-7706eddaca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*298/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a484430-3d93-4985-8769-9cf63ec1b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Slope'][data['Slope']!= '?'].astype(int).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b83f5-905b-4bee-9882-dc72299f1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Slope'][data['Slope']!= '?'].astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2822078-8396-4640-a51f-5cdadf4b03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['Slope'][data['Slope']!= '?'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d326ec-a275-4ab4-8be3-662dc5809f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Slope']== '?'] # before imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78183346-aa52-4b33-87b7-3c898c03d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "df = data.replace('?', np.nan)\n",
    "df = df.apply(pd.to_numeric, errors='ignore')\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "df_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(df),\n",
    "    columns=df.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ed4c5-0cf8-492d-838b-26c9c6a05c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope feature imputation results in non-rounded values which is not an issue for modelling but might conflict with domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c950d28-c184-425a-88c4-df5d497ba5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed[data['Slope']== '?'] # after imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f48b84-fe4a-4765-b1d2-f932d044fad4",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099234b7-0e28-4581-b380-d479b33d0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Facet', 'Aspect']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcce3a0-a28e-4a29-a037-25efb9c92f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def correlation_ratio(categories, measurements):\n",
    "    \"\"\"\n",
    "    (eta) correlation ratio between continuous and categorical features.\n",
    "    Source: https://www.analyticsvidhya.com/blog/2022/08/statistical-effect-size-and-python-implementation\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"cat\": categories, \"meas\": measurements}).dropna()\n",
    "\n",
    "    fcat, _ = pd.factorize(df[\"cat\"])\n",
    "    cat_num = np.max(fcat) + 1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "\n",
    "    for i in range(cat_num):\n",
    "        cat_measures = df[\"meas\"].values[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.mean(cat_measures) if len(cat_measures) > 0 else 0\n",
    "\n",
    "    y_total_avg = np.sum(y_avg_array * n_array) / np.sum(n_array)\n",
    "    numerator = np.sum(n_array * (y_avg_array - y_total_avg) ** 2)\n",
    "    denominator = np.sum((df[\"meas\"].values - y_total_avg) ** 2)\n",
    "\n",
    "    return np.sqrt(numerator / denominator) if denominator != 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6fe48-59a3-401a-b914-4bb09c7939d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_ratio(data['Cover_Type'], data['Facet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584eb02-2d79-40ef-8bce-1e3c5ad34fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_ratio(data['Cover_Type'], data['Aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6375356-e3fd-47a0-8fcc-f20ef7294dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At very least, drop at one. At most, drop both due to low target correlation. Likely drop only one to be cautious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e5e1e-f455-4403-bb31-e76d0f43405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data['Facet'], y=data['Aspect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ce1e3-88ce-4a82-a360-23275e6e9ce4",
   "metadata": {},
   "source": [
    "### Noisy feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b78ede-f17a-4f48-b92f-8b15f3e77978",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_ratio(data['Cover_Type'], data['Inclination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe716c-87f4-4239-b18d-a0fe77a8ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclination has stochastic noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb01ee-357a-41d7-859b-8e5394b61dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['Inclination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b9f00-00d7-41f0-98f9-0a1b96529a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likely remove since the whole feature is noisy and it has basically no correlation to target. Noise removal like through NN or CEWS or iterative partitioning won't help it enough\n",
    "# further, there are already so many features so for the sake of model complexity, it should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a0e0e-f582-480a-84c5-56ce2169a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data['Cover_Type'], y=data['Inclination'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de7d5f-6e28-4a0e-ae0d-b0d5890db7f7",
   "metadata": {},
   "source": [
    "### Features to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d5dc4-2717-40f9-8824-e860dc560a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop water_level and observation_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f14bd-1229-49e0-bd4b-2fb26bda2608",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a2cca-4c30-4bf2-9062-e852ed61b92a",
   "metadata": {},
   "source": [
    "### Metric-related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce3a9b-807c-4067-a9cc-984c6d13639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53703f0-9909-490e-aa69-880bb769d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, roc_curve, roc_auc_score, log_loss\n",
    "def get_results(model, X_test, y_test, y_pred=None, y_prob=None):\n",
    "    \"\"\"\n",
    "    Performs various classification performance metrics on either new or given predictions.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test) if y_pred is None else y_pred\n",
    "    y_prob = model.predict_proba(X_test) if y_prob is None else y_prob\n",
    "    results = {\n",
    "        \"Accuracy\" : accuracy_score(y_test, y_pred),\n",
    "        \"Precision\" : precision_score(y_test, y_pred, average='weighted', zero_division=0.0),\n",
    "        \"Recall\" : recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"F1-score\" : f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"Cohen's kappa\" : cohen_kappa_score(y_test, y_pred),\n",
    "        \"MCC\" : matthews_corrcoef(y_test, y_pred),\n",
    "        \"AUC\" : roc_auc_score(y_test, y_prob, average='weighted', multi_class='ovo'),\n",
    "        \"Cross-Entropy\" : log_loss(y_test, y_prob)\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd286a-314b-4206-9f85-d0489a402666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_dict(data):\n",
    "    \"\"\"\n",
    "    Collapses seperate fold runs into a cohesive dictionary.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    collapsed_data = []\n",
    "\n",
    "    for element in data:\n",
    "        combined = defaultdict(list)\n",
    "        \n",
    "        # Collect all values for each metric\n",
    "        for run in element.values():\n",
    "            for metric, value in run.items():\n",
    "                combined[metric].append(value)\n",
    "        \n",
    "        # Calculate mean and std for each metric\n",
    "        mean_dict = {metric: float(np.mean(values)) for metric, values in combined.items()}\n",
    "        std_dict = {metric: float(np.std(values, ddof=1)) for metric, values in combined.items()}  # sample std\n",
    "        \n",
    "        collapsed_data.append({'mean': mean_dict, 'std': std_dict})\n",
    "    return collapsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eecb04-d741-4b7d-bb81-8be3b846bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(list_of_dicts, hyperparams, x_label, fname):\n",
    "    \"\"\"\n",
    "    Plots metrics as lines against hyperparameter choices.\n",
    "    \"\"\"\n",
    "    all_dicts = collapse_dict(list_of_dicts)\n",
    "    list_of_dicts_mean = [x['mean'] for x in all_dicts]\n",
    "    list_of_dicts_std = [x['std'] for x in all_dicts]\n",
    "    \n",
    "    metrics = list(list_of_dicts_mean[0].keys())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    markers = ['o', 's', '^', 'd', 'v', 'x', '*']\n",
    "    linestyles = ['-', '--', ':', '-.', '-', '--', ':']\n",
    "    num_metrics = len(metrics)\n",
    "    colors = plt.get_cmap(\"viridis\", num_metrics)\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in [\"Cross-Entropy\", \"Precision\", \"Recall\"]: print(metric, np.round(y_values,3)); continue\n",
    "        \n",
    "        y_values = [d[metric] for d in list_of_dicts_mean]\n",
    "        print(metric, np.round(y_values,3))\n",
    "        \n",
    "        y_stds = [d[metric] for d in list_of_dicts_std] \n",
    "\n",
    "        plt.errorbar(hyperparams, y_values, yerr=y_stds, \n",
    "             marker=markers[i % len(markers)], markersize=4,\n",
    "             linestyle=linestyles[i % len(linestyles)], \n",
    "             label=metric, alpha=0.8, color=colors(i),\n",
    "             capsize=3)\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(top=1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{fname}.pdf\", bbox_inches='tight', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b353fa2-33d7-4367-a07c-32e72def72c1",
   "metadata": {},
   "source": [
    "### Preprocessing and division for CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d94dcd-e9b4-4baf-8f85-e4ba11f574f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # z-normalisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "mod_data = data.replace('?', np.nan).drop(columns=[\"Observation_ID\", \"Water_Level\", \"Cover_Type\"])\n",
    "target = data[\"Cover_Type\"]\n",
    "\n",
    "tree_X_train, tree_X_test, tree_y_train, tree_y_test = train_test_split(\n",
    "    mod_data, target, test_size=0.3, random_state=0, stratify=target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb33fb3-645a-4ce6-bd36-6b53f6f1c0f2",
   "metadata": {},
   "source": [
    "### Tree running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5dbf4-4f6e-4782-bca1-f655cf7f2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tree(X, y, criterion='entropy'):\n",
    "    ccp_alphas = np.linspace(0, 0.4, 50)\n",
    "    results = []\n",
    "    for i, ccp_alpha in enumerate(ccp_alphas):\n",
    "        print(f\"{100.0*i/50}%\")\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "        folds = {}\n",
    "        for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "            X_train, y_train, X_val, y_val = X.iloc[train_index], y.iloc[train_index], X.iloc[test_index], y.iloc[test_index]\n",
    "            clf = DecisionTreeClassifier(criterion=criterion, class_weight=\"balanced\", random_state=0, ccp_alpha=ccp_alpha)\n",
    "            clf.fit(X_train, y_train)\n",
    "            folds[fold] = get_results(clf, X_val, y_val)\n",
    "        results.append(folds)\n",
    "    return results, ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f0017-61be-4afc-8360-3fedb199f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tree_results, ccp_alphas = run_tree(tree_X_train, tree_y_train, criterion='entropy')\n",
    "plot_metrics(tree_results, ccp_alphas, \"Cost-Complexity\", \"tree_entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f382895-d818-4cfd-b559-ec9308028db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tree_results, ccp_alphas = run_tree(tree_X_train, tree_y_train, criterion='gini')\n",
    "plot_metrics(tree_results, ccp_alphas, \"Cost-Complexity\", \"tree_gini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f04126-4b33-43c8-8193-802a6a9551e6",
   "metadata": {},
   "source": [
    "### Preprocessing and division for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a39e2-8cb7-426e-b7ba-bceaa48b38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_data = df_imputed.drop(columns=[\"Aspect\", \"Observation_ID\", \"Water_Level\", \"Inclination\", \"Cover_Type\"])\n",
    "target = data[\"Cover_Type\"]\n",
    "\n",
    "scaled_data = mod_data.copy()\n",
    "scaled_data.iloc[:,:10] = StandardScaler().fit_transform(mod_data.iloc[:,:10]) # normalise continuous features only\n",
    "\n",
    "knn_X_train, knn_X_test, knn_y_train, knn_y_test = train_test_split(\n",
    "    scaled_data, target, test_size=0.3, random_state=0, stratify=target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d16288-d125-46d3-bdb6-d3249d031695",
   "metadata": {},
   "source": [
    "### Functions for KNN running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd1f33-17bb-4a03-b1ad-150113c8d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "from pynndescent import NNDescent\n",
    "from scipy.stats import mode\n",
    "from numba import typed\n",
    "# Numba-optimized Gower distance for two vectors\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def gower_distance(x, y, num_indices, cat_indices, ranges, num_features):\n",
    "    dist = 0.0\n",
    "    # Numerical features: normalized Manhattan\n",
    "    for idx, i in enumerate(num_indices):\n",
    "        dist += np.abs(x[i] - y[i]) / ranges[idx]\n",
    "    # Categorical features: Hamming distance\n",
    "    for i in cat_indices:\n",
    "        dist += 1.0 if x[i] != y[i] else 0.0\n",
    "    return dist / num_features\n",
    "    \n",
    "def compute_ranges(X, num_indices):\n",
    "    ranges = np.array([np.ptp(X[:, i]) for i in num_indices], dtype=np.float64)\n",
    "    ranges[ranges == 0] = 1.0  # Avoid division by zero\n",
    "    return ranges\n",
    "\n",
    "class GowerKNNClassifier:\n",
    "    \"\"\"\n",
    "    Special class wrapping NNDescent for this dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors=5, n_jobs=-1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_jobs = n_jobs\n",
    "        self.index = None\n",
    "        self.y_train = None\n",
    "        self.num_indices = None\n",
    "        self.cat_indices = None\n",
    "        self.ranges = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def fit(self, X, y, num_indices, cat_indices, num_features):\n",
    "        self.num_indices = np.array(num_indices, dtype=np.int64)\n",
    "        self.cat_indices = np.array(cat_indices, dtype=np.int64)\n",
    "        self.num_features = num_features\n",
    "        self.y_train = np.array(y)\n",
    "        \n",
    "        self.ranges = compute_ranges(X, num_indices)\n",
    "        self.ranges = np.array(self.ranges, dtype=np.float64)\n",
    "        \n",
    "        self.index = NNDescent(X, metric=gower_distance, metric_kwds={\n",
    "                                \"num_indices\": self.num_indices,\n",
    "                                \"cat_indices\": self.cat_indices,\n",
    "                                \"ranges\": self.ranges,\n",
    "                                \"num_features\": self.num_features},\n",
    "                               n_neighbors=self.n_neighbors, n_jobs=self.n_jobs, random_state=0)\n",
    "        return self\n",
    "\n",
    "    def query(self, X):\n",
    "        # Query nearest neighbors\n",
    "        indices, distances = self.index.query(X, k=self.n_neighbors)\n",
    "        \n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339e9e4-00e5-4c94-8b0f-865f6de516e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_k(distances, indices, y_train, classes, k):\n",
    "    \"\"\"\n",
    "    Slices the given KNN distances to quickly calculate KNN output for different k.\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    y_proba = []\n",
    "    for i in range(len(distances)):\n",
    "        # Slice to top k for this test point\n",
    "        k_dist = distances[i][:k]\n",
    "        k_idx = indices[i][:k]\n",
    "        k_labels = y_train[k_idx]\n",
    "        \n",
    "        # Apply weights\n",
    "        w = 1.0 / (k_dist + 1e-5)  # Avoid division by zero\n",
    "        \n",
    "        # Weighted voting with all classes initialised\n",
    "        class_votes = {c: 0.0 for c in classes}\n",
    "        for label, weight in zip(k_labels, w):\n",
    "            class_votes[label] += weight\n",
    "        \n",
    "        # Compute probabilities\n",
    "        total_weight = sum(class_votes.values())\n",
    "        if total_weight == 0:\n",
    "            probs = [1.0 / len(classes)] * len(classes)  # Uniform if no weights (edge case)\n",
    "        else:\n",
    "            probs = [class_votes[c] / total_weight for c in classes]\n",
    "        \n",
    "        # Predict class (argmax)\n",
    "        pred = classes[np.argmax(probs)]\n",
    "        \n",
    "        y_pred.append(pred)\n",
    "        y_proba.append(probs)\n",
    "    \n",
    "    return np.array(y_pred), np.array(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8568441-14f5-4319-91c3-3e910c21d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_knn(X_train, X_val, y_train, y_val, metric='minkowski', p=2):\n",
    "    use_gower = metric == 'gower'\n",
    "    MAX_K = 100\n",
    "    if use_gower:\n",
    "        X_train = X_train.values\n",
    "        y_train = y_train.values\n",
    "        # Create a partial function with categorical indices fixed\n",
    "        cat_indices = list(range(10, X_train.shape[1]))\n",
    "        num_indices = [i for i in range(X_train.shape[1]) if i not in cat_indices]\n",
    "        \n",
    "        num_features = X_train.shape[1]  # Total features (for averaging)\n",
    "        knn = GowerKNNClassifier(n_neighbors=MAX_K, n_jobs=-1)\n",
    "        knn.fit(X_train, y_train, num_indices, cat_indices, num_features)\n",
    "        return *knn.query(X_val), np.unique(y_train)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=MAX_K, weights='distance', metric=metric, p=p, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Get distances and indices for MAX_K\n",
    "    distances, indices = clf.kneighbors(X_val, return_distance=True)\n",
    "    return distances, indices, clf.classes_# Get the sorted unique classes from the classifier\n",
    "\n",
    "def run_knn(X_train, X_val, y_train, y_val, distances, indices, classes):\n",
    "    results = []\n",
    "    # Test multiple k without recomputing distances\n",
    "    ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 18, 20, 25, 30, 50, 70, 90, 100]\n",
    "    for i, k in enumerate(ks):\n",
    "        print(f\"{100.0*i/len(ks)}%\")\n",
    "        y_pred, y_prob = predict_for_k(distances, indices, y_train.values, classes, k)\n",
    "        results.append(get_results(None, X_val, y_val, y_pred, y_prob))\n",
    "    return results, ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053db71-349c-494a-86c0-9bce93126b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = {}\n",
    "ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 18, 20, 25, 30, 50, 70, 90, 100]\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(knn_X_train, knn_y_train), 1):\n",
    "    X_train, y_train, X_val, y_val = knn_X_train.iloc[train_index], knn_y_train.iloc[train_index], knn_X_train.iloc[test_index], knn_y_train.iloc[test_index]\n",
    "    distances, indices, classes = start_knn(X_train, X_val, y_train, y_val, metric='minkowski', p=2)\n",
    "    knn_results, _ = run_knn(X_train, X_val, y_train, y_val, distances, indices, classes)\n",
    "    folds[fold] = knn_results\n",
    "knn_results = [{key: folds[key][i] for key in folds} for i in range(len(next(iter(folds.values()))))]\n",
    "plot_metrics(knn_results, ks, \"$k$\", \"knn_euclid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8740dc-bd57-4e38-a7fb-bbfc7dade57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = {}\n",
    "ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 18, 20, 25, 30, 50, 70, 90, 100]\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(knn_X_train, knn_y_train), 1):\n",
    "    X_train, y_train, X_val, y_val = knn_X_train.iloc[train_index], knn_y_train.iloc[train_index], knn_X_train.iloc[test_index], knn_y_train.iloc[test_index]\n",
    "    distances, indices, classes = start_knn(X_train, X_val, y_train, y_val, metric='gower')\n",
    "    knn_results, _ = run_knn(X_train, X_val, y_train, y_val, distances, indices, classes)\n",
    "    folds[fold] = knn_results\n",
    "knn_results = [{key: folds[key][i] for key in folds} for i in range(len(next(iter(folds.values()))))]\n",
    "plot_metrics(knn_results, ks, \"$k$\", \"knn_gower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb4168-d8c4-42ca-a48e-f5ff194e7759",
   "metadata": {},
   "source": [
    "### Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79178ed-1602-4482-bc03-fee3432a66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "def plot_roc(y_val, y_scores, fname):\n",
    "    \"\"\"\n",
    "    Plots per-class ROC curves.\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_val)\n",
    "    y_test_bin = label_binarize(y_val, classes=classes)\n",
    "    n_classes = y_test_bin.shape[1]\n",
    "    \n",
    "    fpr_dict = {}\n",
    "    tpr_dict = {}\n",
    "    roc_auc_dict = {}\n",
    "    for i in range(n_classes):\n",
    "        fpr_dict[i], tpr_dict[i], _ = roc_curve(y_test_bin[:, i], y_scores[:, i])\n",
    "        roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
    "    \n",
    "    colors = plt.get_cmap(\"viridis\", n_classes)\n",
    "    plt.figure()\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr_dict[i], tpr_dict[i], label=f'Class {classes[i]} (AUC = {roc_auc_dict[i]:.2f})', color=colors(i))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.7)  # Reference line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.savefig(f\"plots/roc_{fname}.pdf\", bbox_inches='tight', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1a9c6-5bd9-4fd5-a984-23b19651e839",
   "metadata": {},
   "source": [
    "#### CT evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a7405-79f4-48fc-877b-d63a7f2bbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=0, class_weight=\"balanced\")\n",
    "tree.fit(tree_X_train, tree_y_train)\n",
    "tree_y_scores = tree.predict_proba(tree_X_test)\n",
    "tree_y_pred = tree.predict_proba(tree_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738815c0-f62a-47dc-9d4d-53940b78eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(tree, tree_X_test, tree_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c2f78-1207-4488-a370-83e302293752",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(tree_y_test, tree_y_scores, \"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08baa2-aa68-4dda-ac4d-9abd7e4ec759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num 0-1 probability predictions\n",
    "print(f\"{100.0*np.sum(np.all((tree_y_scores == 0) | (tree_y_scores == 1), axis=1))/len(tree_X_test):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9072e-9ee8-4292-a4c4-1998e35541ad",
   "metadata": {},
   "source": [
    "#### KNN evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71976cea-b01b-4496-89db-0ab1da009412",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distances, indices, classes = start_knn(knn_X_train, knn_X_test, knn_y_train, knn_y_test, metric='gower')\n",
    "y_pred, y_scores = predict_for_k(distances, indices, knn_y_train.values, classes, 8)\n",
    "get_results(None, knn_X_test, knn_y_test, y_pred, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e81260-8ff9-4e2f-af76-f6214f429d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(knn_y_test, y_scores, \"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47275133-db4d-4a5d-bb4c-39d91aeb6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{100.0*np.sum(np.all((y_scores == 0) | (y_scores == 1), axis=1))/len(knn_X_test):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaec05-8a9d-42cb-a331-dbdf198830d5",
   "metadata": {},
   "source": [
    "### Training set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d56cf-b5af-4535-a155-534e2219a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=0, class_weight=\"balanced\")\n",
    "tree.fit(tree_X_train, tree_y_train)\n",
    "tree_y_scores = tree.predict_proba(tree_X_train)\n",
    "tree_y_pred = tree.predict_proba(tree_X_train)\n",
    "get_results(tree, tree_X_train, tree_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da35e9-872d-4e52-856a-c1798913721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distances, indices, classes = start_knn(knn_X_train, knn_X_train, knn_y_train, knn_y_train, metric='gower')\n",
    "y_pred, y_scores = predict_for_k(distances, indices, knn_y_train.values, classes, 8)\n",
    "get_results(None, knn_X_train, knn_y_train, y_pred, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dae9ab-ee15-4570-bcfe-9c72deae6b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e4169fc-e120-44e0-88b4-9bd4ad8fc229",
   "metadata": {},
   "source": [
    "### Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4986cc04-974e-4292-b2ba-e9b9ea430f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test(y_true, y_pred_a, y_pred_b):\n",
    "    \"\"\"\n",
    "    Runs McNemar test and finds p-value.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    classes = np.unique(knn_y_test)\n",
    "    if len(y_true.shape) == 1: y_true = label_binarize(y_true, classes=classes)\n",
    "    if len(y_pred_a.shape) == 1: y_pred_a = label_binarize(y_pred_a, classes=classes)\n",
    "    if len(y_pred_b.shape) == 1: y_pred_b = label_binarize(y_pred_b, classes=classes)\n",
    "    a_correct = (np.asarray(y_pred_a) == y_true)\n",
    "    b_correct = (np.asarray(y_pred_b) == y_true)\n",
    "\n",
    "    b = int(np.sum(a_correct & ~b_correct))  # A correct, B wrong\n",
    "    c = int(np.sum(~a_correct & b_correct))  # A wrong, B correct\n",
    "\n",
    "    n = b + c\n",
    "    if n == 0:\n",
    "        # classifiers agree on every sample (both correct or both wrong)\n",
    "        return {'b': b, 'c': c, 'statistic': 0.0, 'pvalue': 1.0}\n",
    "\n",
    "    # continuity corrected chi-square\n",
    "    chi2 = (abs(b - c) - 1)**2 / (b + c)\n",
    "    p = 1 - stats.chi2.cdf(chi2, df=1)\n",
    "    return {'b': b, 'c': c, 'statistic': chi2, 'pvalue': float(p)}\n",
    "\n",
    "def wilcoxon(y_true, prob_a, prob_b, alternative='two-sided'):\n",
    "    \"\"\"\n",
    "    Runs Wilcoxon signed-rank test and finds p-value.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true) - 1 # correct to indices from 1-based labels\n",
    "    prob_a = np.asarray(prob_a)\n",
    "    prob_b = np.asarray(prob_b)\n",
    "\n",
    "    # If 2D arrays, extract predicted probability for the true class\n",
    "    if prob_a.ndim == 2:\n",
    "        p_true_a = prob_a[np.arange(len(y_true)), y_true]\n",
    "    elif prob_a.ndim == 1:\n",
    "        p_true_a = prob_a\n",
    "\n",
    "    if prob_b.ndim == 2:\n",
    "        p_true_b = prob_b[np.arange(len(y_true)), y_true]\n",
    "    elif prob_b.ndim == 1:\n",
    "        p_true_b = prob_b\n",
    "\n",
    "    diffs = p_true_a - p_true_b\n",
    "    print(f\"Median difference: {np.median(diffs)}\")\n",
    "    print(f\"Mean difference: {np.mean(diffs)}\")\n",
    "    nonzero = diffs != 0\n",
    "    n_nonzero = int(np.sum(nonzero))\n",
    "    if n_nonzero == 0:\n",
    "        return {'statistic': 0.0, 'pvalue': 1.0, 'n_nonzero': 0}\n",
    "\n",
    "    stat, p = stats.wilcoxon(diffs[nonzero], alternative=alternative)\n",
    "    return {'statistic': float(stat), 'pvalue': float(p), 'n_nonzero': n_nonzero}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d743d19-af79-4064-b798-734c24745571",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_res = mcnemar_test(knn_y_test, tree_y_pred, y_pred)\n",
    "\n",
    "w_res = wilcoxon(knn_y_test, tree_y_scores, y_scores, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200eccc-a9f9-40fd-9d62-2c817262448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647516c-b8b7-4761-8c10-e48c8dfc48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d11fee-6423-40cb-b33b-15e67a2af24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
